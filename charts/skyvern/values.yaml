  # Common configuration
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Deployment strategy
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%

# Service Account
serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

# Pod configurationss
podAnnotations: {}
podLabels: {}
podSecurityContext: {}
securityContext: {}

# Job configuration : AJOUTE EN DUR CAR NE FONCTIONNE PAS AVEC LE RANGE HELM
# job:
#   browserFactory:
#     file: "/app/skyvern/webeye/browser_factory.py"
#     insertAfter: "browser_args = ["
#     content:
#       - "            \"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\","
#       - "            \"--disable-notifications\","

livenessProbe:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1

postgresql:
  enabled: true
  architecture: standalone
  auth:
    username: "skyvern"
    password: "skyvern"
    database: "skyvern"
  primary:
    service:
      ports:
        postgresql: 5432
    persistence:
      size: 1Gi
      enabled: true
      existingClaim: ""
    containerSecurityContext:
      runAsUser: 1001
      runAsNonRoot: true
    podSecurityContext:
      fsGroup: 1001
      runAsUser: 1001

skyvern:
  image:
    repository: public.ecr.aws/skyvern/skyvern
    tag: latest
  service:
    type: ClusterIP
    port: 8000
  persistence:
    artifacts:
      enabled: true
      size: 1Gi
    videos:
      enabled: true
      size: 1Gi
    har:
      enabled: true
      size: 1Gi
    log:
      enabled: true
      size: 1Gi
    streamlit:
      enabled: true
      size: 100Mi
  env:
    BROWSER_TYPE: "chromium-headless"
    ENABLE_OPENAI: "true"
    LLM_KEY: "OPENAI_GPT4O_MINI"
    SECONDARY_LLM_KEY: "OPENAI_GPT4O_MINI"
    SKYVERN_TELEMETRY: "false"
    LOG_LEVEL: "DEBUG"
    BROWSER_LOCALE: "fr-FR"
    BROWSER_TIMEZONE: "Europe/Paris"
    # If you want to use other LLM provider, like azure and anthropic:
    # - ENABLE_ANTHROPIC=true
    # - LLM_KEY=ANTHROPIC_CLAUDE3.5_SONNET
    # Microsoft Azure OpenAI support:
    # If you'd like to use Microsoft Azure OpenAI as your managed LLM service integration with Skyvern, use the environment variables below.
    # In your Microsoft Azure subscription, you will need to provision the OpenAI service and deploy a model, in order to utilize it.
    # 1. Login to the Azure Portal
    # 2. Create an Azure Resource Group
    # 3. Create an OpenAI resource in the Resource Group (choose a region and pricing tier)
    # 4. From the OpenAI resource's Overview page, open the "Azure AI Foundry" portal (click the "Explore Azure AI Foundry Portal" button)
    # 5. In Azure AI Foundry, click "Shared Resources" --> "Deployments"
    # 6. Click "Deploy Model" --> "Deploy Base Model" --> select a model (specify this model "Deployment Name" value for the AZURE_DEPLOYMENT variable below)
    # - ENABLE_AZURE=true
    # - LLM_KEY=AZURE_OPENAI                          # Leave this value static, don't change it
    # - AZURE_DEPLOYMENT=<your_azure_deployment>      # Use the OpenAI model "Deployment Name" that you deployed, using the steps above
    # - AZURE_API_KEY=<your_azure_api_key>            # Copy and paste Key1 or Key2 from the OpenAI resource in Azure Portal
    # - AZURE_API_BASE=<your_azure_api_base>          # Copy and paste the "Endpoint" from the OpenAI resource in Azure Portal (eg. https://xyzxyzxyz.openai.azure.com/)
    # - AZURE_API_VERSION=<your_azure_api_version>    # Specify a valid Azure OpenAI data-plane API version (eg. 2024-08-01-preview) Docs: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference
    # Amazon Bedrock Support:
    # Amazon Bedrock is a managed service that enables you to invoke LLMs and bill them through your AWS account.
    # To use Amazon Bedrock as the LLM provider for Skyvern, specify the following environment variables.
    # 1. In the AWS IAM console, create a new AWS IAM User (name it whatever you want)
    # 2. Assign the "AmazonBedrockFullAccess" policy to the user
    # 3. Generate an IAM Access Key under the IAM User's Security Credentials tab
    # 4. In the Amazon Bedrock console, go to "Model Access"
    # 5. Click Modify Model Access button
    # 6. Enable "Claude 3.5 Sonnet v2" and save changes
    # - ENABLE_BEDROCK=true
    # - LLM_KEY=BEDROCK_ANTHROPIC_CLAUDE3.5_SONNET   # This is the Claude 3.5 Sonnet "V2" model. Change to BEDROCK_ANTHROPIC_CLAUDE3.5_SONNET_V1 for the non-v2 version.
    # - AWS_REGION=us-west-2                         # Replace this with a different AWS region, if you desire
    # - AWS_ACCESS_KEY_ID=FILL_ME_IN_PLEASE
    # - AWS_SECRET_ACCESS_KEY=FILL_ME_IN_PLEASE

skyvernUi:
  image:
    repository: public.ecr.aws/skyvern/skyvern-ui
    tag: latest
  service:
    type: ClusterIP
    port: 8080
    artifactPort: 9090
  env:
    VITE_WSS_BASE_URL: wss://skyvern.allapps.me/api/v1
    VITE_API_BASE_URL: https://skyvern.allapps.me/api/v1
    VITE_ARTIFACT_API_BASE_URL: https://skyvern.allapps.me
    VITE_SKYVERN_API_KEY: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjQ4ODQ5MjEwMTEsInN1YiI6Im9fMzYxOTcwNDQ2MDk4MTU0Mjg4In0.afTIQN7FmC-cLRyKZpnyK2M0uNQKx_R8uExzhU0Gx3Q

ingress:
  enabled: true
  entryPoints: web
  annotations:
    kubernetes.io/ingress.class: traefik
