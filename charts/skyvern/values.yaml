  # Common configuration
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Deployment strategy
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 25%
    maxUnavailable: 25%

# Service Account
serviceAccount:
  create: true
  automount: true
  annotations: {}
  name: ""

# Pod configurations
podAnnotations: {}
podLabels: {}
podSecurityContext: {}
securityContext: {}

postgresql:
  enabled: true
  architecture: standalone
  auth:
    username: "skyvern"
    password: "skyvern"
    database: "skyvern"
  primary:
    service:
      ports:
        postgresql: 5432
    persistence:
      size: 1Gi
      enabled: true
      existingClaim: ""
    containerSecurityContext:
      runAsUser: 1001
      runAsNonRoot: true
    podSecurityContext:
      fsGroup: 1001
      runAsUser: 1001

skyvern:
  image:
    repository: public.ecr.aws/skyvern/skyvern
    tag: latest
  service:
    type: ClusterIP
    port: 8000
  persistence:
    artifacts:
      enabled: true
      size: 1Gi
    videos:
      enabled: true
      size: 1Gi
    har:
      enabled: true
      size: 1Gi
    log:
      enabled: true
      size: 1Gi
    streamlit:
      enabled: true
      size: 100Mi
  env:
    BROWSER_TYPE: "chromium-headless"
    ENABLE_OPENAI: "true"
    OPENAI_API_KEY: ""

    # If you want to use other LLM provider, like azure and anthropic:
    # - ENABLE_ANTHROPIC=true
    # - LLM_KEY=ANTHROPIC_CLAUDE3.5_SONNET
    # - ANTHROPIC_API_KEY=<your_anthropic_key>
    # Microsoft Azure OpenAI support:
    # If you'd like to use Microsoft Azure OpenAI as your managed LLM service integration with Skyvern, use the environment variables below.
    # In your Microsoft Azure subscription, you will need to provision the OpenAI service and deploy a model, in order to utilize it.
    # 1. Login to the Azure Portal
    # 2. Create an Azure Resource Group
    # 3. Create an OpenAI resource in the Resource Group (choose a region and pricing tier)
    # 4. From the OpenAI resource's Overview page, open the "Azure AI Foundry" portal (click the "Explore Azure AI Foundry Portal" button)
    # 5. In Azure AI Foundry, click "Shared Resources" --> "Deployments"
    # 6. Click "Deploy Model" --> "Deploy Base Model" --> select a model (specify this model "Deployment Name" value for the AZURE_DEPLOYMENT variable below)
    # - ENABLE_AZURE=true
    # - LLM_KEY=AZURE_OPENAI                          # Leave this value static, don't change it
    # - AZURE_DEPLOYMENT=<your_azure_deployment>      # Use the OpenAI model "Deployment Name" that you deployed, using the steps above
    # - AZURE_API_KEY=<your_azure_api_key>            # Copy and paste Key1 or Key2 from the OpenAI resource in Azure Portal
    # - AZURE_API_BASE=<your_azure_api_base>          # Copy and paste the "Endpoint" from the OpenAI resource in Azure Portal (eg. https://xyzxyzxyz.openai.azure.com/)
    # - AZURE_API_VERSION=<your_azure_api_version>    # Specify a valid Azure OpenAI data-plane API version (eg. 2024-08-01-preview) Docs: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference
    # Amazon Bedrock Support:
    # Amazon Bedrock is a managed service that enables you to invoke LLMs and bill them through your AWS account.
    # To use Amazon Bedrock as the LLM provider for Skyvern, specify the following environment variables.
    # 1. In the AWS IAM console, create a new AWS IAM User (name it whatever you want)
    # 2. Assign the "AmazonBedrockFullAccess" policy to the user
    # 3. Generate an IAM Access Key under the IAM User's Security Credentials tab
    # 4. In the Amazon Bedrock console, go to "Model Access"
    # 5. Click Modify Model Access button
    # 6. Enable "Claude 3.5 Sonnet v2" and save changes
    # - ENABLE_BEDROCK=true
    # - LLM_KEY=BEDROCK_ANTHROPIC_CLAUDE3.5_SONNET   # This is the Claude 3.5 Sonnet "V2" model. Change to BEDROCK_ANTHROPIC_CLAUDE3.5_SONNET_V1 for the non-v2 version.
    # - AWS_REGION=us-west-2                         # Replace this with a different AWS region, if you desire
    # - AWS_ACCESS_KEY_ID=FILL_ME_IN_PLEASE
    # - AWS_SECRET_ACCESS_KEY=FILL_ME_IN_PLEASE

skyvernUi:
  image:
    repository: public.ecr.aws/skyvern/skyvern-ui
    tag: latest
  service:
    type: ClusterIP
    port: 8080
    wsPort: 9090
  env:
    VITE_WSS_BASE_URL: ws://allapps.me/api/v1
    VITE_API_BASE_URL: https://allapps.me//api/v1
    VITE_ARTIFACT_API_BASE_URL: https://allapps.me/api/v1/stream

ingress:
  enabled: true
  entryPoints: web
  # All routes will be prefixed with this path (e.g., /skyvern)
  basePath: /skyvern
  middlewares:
  - name: skyvern-strip-prefix
  annotations:
    kubernetes.io/ingress.class: traefik
